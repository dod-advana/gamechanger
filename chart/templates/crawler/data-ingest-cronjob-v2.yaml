# using the Dockerfile found in gamechanger-crawlers/dataPipelines/paasJobs/docker/
# values.yaml should have crawler.cronjob.crons array of crawlers to run with schedule, name, id, scriptFile etc.
{{- range .Values.crawler.cronJobs }}
---
apiVersion: batch/v1
kind: CronJob
metadata:
  namespace: {{ $.Release.Namespace }}
spec:
  concurrencyPolicy: "Allow"
  failedJobsHistoryLimit: 1
  schedule: {{ .schedule }}
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: {{ .name }}-cronjob
            - image: {{ $.Values.crawler.image.repository }}
            #- image:{{/* - include "app.crawler.image" . | nindent 4 */}} # gamechanger image
              env:
                - name: "parser_threads"
                  value: {{ .parser_threads | default 68 | int }}
                - name: "max_ocr_threads"
                  value: {{ .max_ocr_threads | default 1 | int }}
                - name: "skip_neo4j"
                  value: {{ .skip_neo4j | default "no" | quote}}
                - name: "flockfile"
                  value: {{ .flockfile | default "/data/run/gamechanger-crawler-ingest.lock" | quote }}
                - name: "cmd"
                  value: {{ .cmd | default "/data/gamechanger/gamechanger-data/paasJobs/job_runner.sh" | quote }}
                - name: "arg1"
                  value: {{ .arg1 | default "/data/gamechanger/gamechanger-data/paasJobs/jobs/configs/crawler_ingest.conf.sh" | quote }}
              command: 
              - "/bin/sh"
              - -c
              - env LOCAL_SPIDER_LIST_FILE="etc/crawler_schedule/{{ .name }}.txt"
              - JOB_NAME={{ .name }}
              - MAX_OCR_THREADS_PER_FILE=$(max_ocr_threads) 
              - MAX_PARSER_THREADS=$(parser_threads) 
              - SKIP_NEO4J_UPDATE=$(skip_neo4j) 
              - $(cmd) 
              - $(arg1)
              # restartPolicy: {{ .restartPolicy }}
              volumeMounts:
                - name: local_spider_list
                  mountPath: /etc/crawler_schedule
          volumes:
            - name: local_spider_list
              configMap:
                name: Crawler-Configmap
  successfulJobsHistoryLimit: 3
  suspend: false
{{- end }}
