# using the Dockerfile found in gamechanger-crawlers/dataPipelines/paasJobs/docker/
# values.yaml should have crawler.cronjob.crons array of crawlers to run with schedule, name, id, scriptFile etc.
{{- $defaultImage := include "app.crawlers.default.image" . }}
{{- $defaultJobConfigs := $.Values.crawlers.defaultJobConfigs }}
{{- range .Values.crawlers.cronJobs }}
---
apiVersion: {{ include "common.capabilities.cronjob.apiVersion" $ }}
kind: CronJob
metadata:
  namespace: {{ $.Release.Namespace }}
  name: {{ .name }}
spec:
  concurrencyPolicy: "Allow"
  failedJobsHistoryLimit: 1
  schedule: {{ .schedule }}
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: {{ include "app.crawlers.restartPolicy" (dict "current" . "default" $defaultJobConfigs) | quote }}
          # init container to pull latest aws-cli cacerts.pem and combine with
          # configmap's cacerts.pem, then store in /tmp directory as shared volume with
          # the cronjob containers
          initContainers:
            - name: append-certs
              image: {{ template "app.crawlers.init.image" $ }}
              command:
                - "/bin/sh"
                - "-c"
                - "cat" 
                - "/etc/certificates/cacert.pem"
                - "/usr/local/aws-cli/v2/current/dist/awscli/botocore/cacert.pem"
                - ">"
                - "/tmp/certificates/cacert.pem"
              volumeMounts:
                - name: tmp
                  mountPath: /tmp/certificates
                - name: certificates-configmap
                  mountPath: /etc/certificates
            - name: {{ .name }}-crawl-download
              image: {{ template "app.crawlers.image" (dict "current" . "default" $defaultImage) }}
              command: 
                - "crawl"
              env: 
                - name: "GC_CRAWL_SPIDERS_FILE_LOCATION"
                  value: "/etc/crawler_schedule/{{ .name }}.txt"
                - name: "GC_CRAWL_DOWNLOAD_OUTPUT_DIR" 
                  value: {{ include "app.crawlers.LOCAL_DOWNLOAD_DIRECTORY_PATH" (dict "current" . "default" $defaultJobConfigs) | quote }}
                - name: "GC_CRAWL_PREVIOUS_MANIFEST_LOCATION"
                  value: {{ include "app.crawlers.LOCAL_PREVIOUS_MANIFEST_LOCATION" (dict "current" . "default" $defaultJobConfigs) | quote }}
                - name: "GC_CRAWL_SKIP_CONFIRMATION"
                  value: "y"
                - name: "GC_CRAWL_CRAWLER_OUTPUT_LOCATION"
                  value: {{ include "app.crawlers.CRAWLER_OUTPUT_LOCATION" (dict "current" . "default" $defaultJobConfigs) | quote }}
          containers:
            - name: {{ .name }}-scanner
              # image for this cronjob should be from gamechanger-crawlers/paasJobs/docker/crawl_and_download/Dockerfile
              image: {{ template "app.crawlers.image" (dict "current" . "default" $defaultImage) }}
              env:
                  # SCAN Path should be same as LOCAL_DOWNLOAD_DIRECTORY_PATH? Long-running job, might want to separate download paths for performance
                - name: "GC_SCAN_INPUT_PATH" 
                  value: {{ include "app.crawlers.LOCAL_DOWNLOAD_DIRECTORY_PATH" (dict "current" . "default" $defaultJobConfigs) | quote }}
                  # yes|no
                - name: "DELETE_AFTER_UPLOAD"
                  value: {{ include "app.crawlers.DELETE_AFTER_UPLOAD" (dict "current" . "default" $defaultJobConfigs) | quote }}
                  # Variables below for slack notification config - used in job_runner.sh
                  # Could move to a configmap and include using envFrom, slack hook URL should also be secret
                # - name: "SEND_NOTIFICATIONS"
                #   value: {{ include "app.crawlers.SEND_NOTIFICATIONS" (dict "current" . "default" $defaultJobConfigs) | quote }}
                # - name: "SLACK_HOOK_CHANNEL"
                #   value: {{ include "app.crawlers.SLACK_HOOK_CHANNEL" (dict "current" . "default" $defaultJobConfigs) | quote }}
                # - name: "SLACK_HOOK_URL"
                #   valueFrom:
                #     secretKeyRef:
                #       name: {{ include "app.crawlers.secretName" $ }}
                #       key: "SLACK_HOOK_URL"

                  # S3 base path for final uploads sans bucket name
                - name: "S3_UPLOAD_BASE_PATH"
                  value: {{ include "app.crawlers.S3_UPLOAD_BASE_PATH" (dict "current" . "default" $defaultJobConfigs) | quote }}
                  # S3 bucket name
                - name: "BUCKET"
                  value: 
                  # yes|no
                - name: "SKIP_S3_UPLOAD"
                  value: {{ include "app.crawlers.SKIP_S3_UPLOAD" (dict "current" . "default" $defaultJobConfigs) | quote }}
              volumeMounts:
                - name: "local-spider-list"
                  mountPath: "/etc/crawler_schedule"
                - name: "tmp"
                  mountPath: "/tmp/certificates"
                - name: {{ include "app.crawlers.downloadDirectoryVolumeConfig.name" (dict "current" . "default" $defaultJobConfigs) | quote }}
                  mountPath: {{ include "app.crawlers.downloadDirectoryVolumeConfig.mountPath" (dict "current" . "default" $defaultJobConfigs) | quote }}
          volumes:
            - name: "tmp"
              emptyDir: {}
            - name: "local-spider-list"
              configMap:
                name: "crawlers-configmap"
            - name: "certificates-configmap"
              configMap:
                name: "certificates-configmap"
            - name: {{ include "app.crawlers.downloadDirectoryVolumeConfig.name" (dict "current" . "default" $defaultJobConfigs) | quote }}
              persistentVolumeClaim:
                claimName: {{ include "app.crawlers.downloadDirectoryVolumeConfig.claimName" (dict "current" . "default" $defaultJobConfigs) | quote }}
            
  successfulJobsHistoryLimit: 3
  suspend: false
{{- end }}
